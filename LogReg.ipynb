{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b780b34-8386-4749-9251-5081773fec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "554de671-0cc6-4ae1-acc4-015f50dade93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_filename = 'songs_dataset_train.pkl'\n",
    "with open(pickle_filename, 'rb') as file:\n",
    "    train_df = pickle.load(file)\n",
    "\n",
    "\n",
    "pickle_filename = 'songs_dataset_test.pkl'\n",
    "with open(pickle_filename, 'rb') as file:\n",
    "    test_df = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "num_train = train_df.drop(['Lyrics', 'Title', 'Raw_Lyrics'], axis=1)\n",
    "num_test = test_df.drop(['Lyrics', 'Title', 'Raw_Lyrics'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "artist_list_file = \"artist_list.pkl\"\n",
    "\n",
    "# Load the 'artists' list from the saved file\n",
    "with open(artist_list_file, 'rb') as file:\n",
    "    artists = pickle.load(file)\n",
    "\n",
    "arts_dict = {}\n",
    "for i in range(len(artists)):\n",
    "    arts_dict[artists[i]] = i\n",
    "    \n",
    "arts_dict_op = {i: artist for artist, i in arts_dict.items()}\n",
    "\n",
    "arts_in_dataset = set(train_df['Artist'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f905c689-7cd9-4993-8a79-8c261e0a56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_sets_func(df, art_set):\n",
    "    \"\"\"\n",
    "    Filter and preprocess a DataFrame based on a given artist set.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame containing artist-related data.\n",
    "        art_set (list): A list of artists to filter the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the following elements:\n",
    "            - df2 (DataFrame): The filtered DataFrame with specified columns.\n",
    "            - fts (Index): The column names of the filtered DataFrame.\n",
    "            - pos_w (float): The positive weight for each artist set (1 - 1/n_art).\n",
    "            - neg_w (float): The negative weight for each artist set (1/n_art).\n",
    "    \"\"\"\n",
    "    df2 = df.drop(df.columns[[0, 3, 4]], axis=1)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    df2 = df2[df2['Artist'].isin(art_set)]\n",
    "    n_art = len(art_set)\n",
    "    pos_w = (n_art - 1) / n_art\n",
    "    neg_w = 1 / n_art\n",
    "    fts = df2.columns\n",
    "    \n",
    "    return df2, fts, pos_w, neg_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c56aa7e2-6da5-429c-949f-82c62bcb1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_arrays(matrix):\n",
    "    \"\"\"\n",
    "    Calculate the mean values along each column of a matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix (numpy.ndarray): The input matrix for which to calculate means.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of mean values for each column of the input matrix.\n",
    "    \"\"\"\n",
    "    mean_values = np.mean(matrix, axis=0)\n",
    "    return mean_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6058b0ba-f331-4617-bc83-5603ee2b5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist_df(artist, df):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation of features for songs by a specific artist in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        artist : The name or identifier of the artist.\n",
    "        df (DataFrame): The input DataFrame containing song data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the following elements:\n",
    "            - artist_stats (DataFrame): A DataFrame with the average and standard deviation of features for the artist's songs.\n",
    "            - mean_vector (numpy.ndarray): The mean TF-IDF vector of the artist's songs.\n",
    "    \"\"\"\n",
    "    art_data = df[df['Artist'] == artist]\n",
    "    arrays = art_data['TFIDF_Vector'].values\n",
    "    matrix = np.stack(arrays)\n",
    "    mean_vector = np.array(np.mean(matrix, axis=0))\n",
    "    average_features_artist = art_data.mean(numeric_only=True)\n",
    "    std_features_artist = art_data.std(numeric_only=True)\n",
    "    artist_stats = pd.DataFrame({'Average': average_features_artist, 'Standard Deviation': std_features_artist})\n",
    "    artist_stats = artist_stats.transpose().drop(\"Artist\", axis=1)\n",
    "    \n",
    "    return artist_stats, mean_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b4cbb061-6b65-4fd1-880a-d1315b43718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_angle(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vector1 (numpy.ndarray): The first vector.\n",
    "        vector2 (numpy.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "        float: A similarity score between 0 and 1, where 1 indicates high similarity.\n",
    "        \n",
    "    Note:\n",
    "        This function is typically used for calculating the cosine similarity\n",
    "        between TF-IDF vectors or other vectors in the context of text analysis\n",
    "        or recommendation systems.\n",
    "    \"\"\"\n",
    "    dot_prod = np.dot(vector1, vector2)\n",
    "    mag1 = np.linalg.norm(vector1)\n",
    "    mag2 = np.linalg.norm(vector2)\n",
    "    if mag1 * mag2 != 0:\n",
    "        return dot_prod / (mag1 * mag2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "81c3686f-cae3-47b2-b3f8-9a407003095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_rows(df):\n",
    "    \"\"\"\n",
    "    Takes a dataframe with the first 2 rows belonging to an artist and the following rows representing songs.\n",
    "    Returns the dataframe to be trained for that artist.\n",
    "    \"\"\"\n",
    "    art_pca = df.iloc[0, -1]\n",
    "\n",
    "    for i in range(2, len(df)):\n",
    "        df.iloc[i, -1] = cosine_angle(art_pca, df.iloc[i, -1])\n",
    "\n",
    "    first_row = df.iloc[0, :-1]\n",
    "    s_row = df.iloc[1, :-1]\n",
    "\n",
    "    mask = s_row != 0  # Create a mask to avoid division by zero\n",
    "    # Convert the values to float64 before assigning them to the DataFrame\n",
    "    df.iloc[2:, :-1] = (first_row - df.iloc[2:, :-1]) / np.where(mask, s_row, 1)\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ac4214b1-1702-4201-a458-e3c98b89871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Will make the dataframe of songs for any artist\n",
    "def df_maker(artist, df, df2):\n",
    "    \"\"\" \n",
    "    INPUTS\n",
    "    artist- aritst number, int\n",
    "    df- dataframe with songs and their features\n",
    "    output- dataframe with featurs of the song wrt the aritst\n",
    "    \"\"\"\n",
    "    art_df, meanpca = artist_df(artist, df2)\n",
    "    art_df[\"TFIDF_Vector\"] = [meanpca, None]\n",
    "    comb_df = pd.concat([art_df, df])\n",
    "    return (std_rows(comb_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "853d6bf7-fe99-43ee-882e-504e486f641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_K_accs(y_test, arr):\n",
    "    \"\"\"\n",
    "    Calculate the top-K accuracy scores for a list of predicted indices.\n",
    "\n",
    "    Args:\n",
    "        y_test (list or numpy.ndarray): The true labels or ground truth.\n",
    "        arr (list of lists): A list of lists, where each sublist contains the top-K predicted indices.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of top-K accuracy scores, one for each sublist in 'arr'.\n",
    "\n",
    "    Note:\n",
    "        Top-K accuracy measures the percentage of correctly predicted labels\n",
    "        when considering the top-K predicted labels for each sample. It is useful\n",
    "        for evaluating classification models that provide multiple predictions\n",
    "        for each sample.\n",
    "    \"\"\"\n",
    "    y_test = np.array(y_test)\n",
    "    l = len(y_test)\n",
    "    top_accs = []\n",
    "    \n",
    "    for max_indices in arr:\n",
    "        s = 0\n",
    "        for i in range(0, l):\n",
    "            if y_test[i] in max_indices[i]:\n",
    "                s += 1\n",
    "        top_accs.append(100 * s / l)\n",
    "    \n",
    "    return top_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b88bed95-4bfe-4f4e-8830-3ccf3bc2f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_test(artists_set, top, x_train, x_test):\n",
    "    \"\"\"\n",
    "    Perform logistic regression-based artist classification and evaluate top-K accuracy.\n",
    "\n",
    "    Args:\n",
    "        artists_set (list): List of artists to classify.\n",
    "        top (list): List of integers specifying the top-K values for accuracy evaluation.\n",
    "        x_train (DataFrame): Training feature data.\n",
    "        x_test (DataFrame): Testing feature data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the following elements:\n",
    "            - probs_by_tops (list of lists): List of top-K predicted artists for each sample.\n",
    "            - top_K_accuracy (list): List of top-K accuracy scores for each top-K value in 'top'.\n",
    "            - coefficients (numpy.ndarray): Mean coefficients from logistic regression models.\n",
    "            - y_test (numpy.ndarray): True labels from the testing data.\n",
    "            - fts (Index): Feature column names.\n",
    "            - artists_set (list): List of artists used in the classification.\n",
    "\n",
    "    Note:\n",
    "        This function performs logistic regression-based artist classification for a given\n",
    "        set of artists and evaluates the top-K accuracy of the predictions. It also returns\n",
    "        the mean coefficients, true labels, feature column names, and the artists in the set.\n",
    "    \"\"\"\n",
    "    # Filter the training data for the specified artist set\n",
    "    df2, _, pos_w, neg_w = n_sets_func(df=x_train, art_set=artists_set)\n",
    "    x_train = df2.drop('Artist', axis=1)\n",
    "    y_train = df2['Artist']\n",
    "\n",
    "    # Filter the testing data for the specified artist set\n",
    "    df3, _, pos_w, neg_w = n_sets_func(df=x_test, art_set=artists_set)\n",
    "    x_test = df3.drop('Artist', axis=1)\n",
    "    y_test = df3['Artist']\n",
    "    fts = x_train.columns\n",
    "\n",
    "    probs = []\n",
    "    coeffs = []\n",
    "\n",
    "    for i in artists_set:\n",
    "        X2 = df_maker(artist=i, df=x_train, df2=df2)\n",
    "        X2 = pd.concat([X2.reset_index(drop=True), x_train.iloc[:, :-1].reset_index(drop=True)], axis=1)\n",
    "        X3 = df_maker(artist=i, df=x_test, df2=df2)\n",
    "        X3 = pd.concat([X3.reset_index(drop=True), x_test.iloc[:, :-1].reset_index(drop=True)], axis=1)\n",
    "        y2 = np.array([1 if x == i else 0 for x in y_train])\n",
    "\n",
    "        # Fit logistic regression model\n",
    "        logreg = LogisticRegression(max_iter=5000, solver='lbfgs', class_weight={0: neg_w, 1: pos_w})\n",
    "        logreg.fit(X2, y2)\n",
    "        coefficients = logreg.coef_\n",
    "        coeffs.append(coefficients[0])\n",
    "        probs.append(logreg.predict_proba(X3)[:, 1])\n",
    "\n",
    "    coeffs = np.vstack(coeffs)\n",
    "    coeffs = np.mean(coeffs, axis=0)\n",
    "    probs = np.array(probs)\n",
    "    probs_by_tops = []\n",
    "\n",
    "    for i in top:\n",
    "        Q = (np.argsort(probs, axis=0)[-i:, :]).transpose()\n",
    "        temp = []\n",
    "        for i in Q:\n",
    "            temp2 = []\n",
    "            for j in i:\n",
    "                temp2.append(artists_set[j])\n",
    "            temp.append(temp2)\n",
    "        probs_by_tops.append(temp)\n",
    "\n",
    "    return probs_by_tops, top_K_accs(y_test=y_test, arr=probs_by_tops), coeffs, y_test, fts, artists_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "84bda8de-8233-4697-ae07-74ed979089f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_train_acc(artists_set, top, x_train):\n",
    "    \"\"\"\n",
    "    Calculate top-K accuracy for logistic regression training on a given artist set.\n",
    "\n",
    "    Args:\n",
    "        artists_set (list): List of artists for training.\n",
    "        top (list): List of integers specifying the top-K values for accuracy evaluation.\n",
    "        x_train (DataFrame): Training feature data.\n",
    "\n",
    "    Returns:\n",
    "        list: List of top-K accuracy scores for each top-K value in 'top'.\n",
    "\n",
    "    Note:\n",
    "        This function performs logistic regression-based training for a given\n",
    "        set of artists and evaluates the top-K accuracy of the training predictions.\n",
    "    \"\"\"\n",
    "    # Filter the training data for the specified artist set\n",
    "    df2, _, pos_w, neg_w = n_sets_func(df=x_train, art_set=artists_set)\n",
    "    x_train = df2.drop('Artist', axis=1)\n",
    "    y_train = df2['Artist']\n",
    "\n",
    "    probs = []\n",
    "\n",
    "    for i in artists_set:\n",
    "        X2 = df_maker(artist=i, df=x_train, df2=df2)\n",
    "        X2 = pd.concat([X2.reset_index(drop=True), x_train.iloc[:, :-1].reset_index(drop=True)], axis=1)\n",
    "        y2 = np.array([1 if x == i else 0 for x in y_train])\n",
    "\n",
    "        # Fit logistic regression model\n",
    "        logreg = LogisticRegression(max_iter=5000, solver='lbfgs', class_weight={0: neg_w, 1: pos_w})\n",
    "        logreg.fit(X2, y2)\n",
    "        probs.append(logreg.predict_proba(X2)[:, 1])\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    probs_by_tops = []\n",
    "\n",
    "    for i in top:\n",
    "        Q = (np.argsort(probs, axis=0)[-i:, :]).transpose()\n",
    "        temp = []\n",
    "        for i in Q:\n",
    "            temp2 = []\n",
    "            for j in i:\n",
    "                temp2.append(artists_set[j])\n",
    "            temp.append(temp2)\n",
    "        probs_by_tops.append(temp)\n",
    "\n",
    "    return top_K_accs(y_test=y_train, arr=probs_by_tops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e0673b77-3a38-4d08-b211-53d5e730d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_test_random_bagging(artists_set, top, x_train, x_test, nest, max_sam, max_fts, oob, warm):\n",
    "    \"\"\"\n",
    "    Perform logistic regression-based artist classification with bagging and evaluate top-K accuracy.\n",
    "\n",
    "    Args:\n",
    "        artists_set (list): List of artists to classify.\n",
    "        top (list): List of integers specifying the top-K values for accuracy evaluation.\n",
    "        x_train (DataFrame): Training feature data.\n",
    "        x_test (DataFrame): Testing feature data.\n",
    "        nest (int): Number of base estimators in the bagging ensemble.\n",
    "        max_sam (float or int): The number of samples to draw from X2 to train each base estimator.\n",
    "        max_fts (float or int): The number of features to draw from X2 to train each base estimator.\n",
    "        oob (bool): Whether to use out-of-bag samples to estimate the generalization error.\n",
    "        warm (bool): Whether to reuse the solution of the previous call to fit and add more estimators to the ensemble.\n",
    "\n",
    "    Returns:\n",
    "        list: List of top-K accuracy scores for each top-K value in 'top'.\n",
    "\n",
    "    Note:\n",
    "        This function performs logistic regression-based artist classification with bagging\n",
    "        for a given set of artists and evaluates the top-K accuracy of the predictions.\n",
    "    \"\"\"\n",
    "    # Filter the training data for the specified artist set\n",
    "    df2, _, pos_w, neg_w = n_sets_func(df=x_train, art_set=artists_set)\n",
    "    x_train = df2.drop('Artist', axis=1)\n",
    "    y_train = df2['Artist']\n",
    "\n",
    "    # Filter the testing data for the specified artist set\n",
    "    df3, _, pos_w, neg_w = n_sets_func(df=x_test, art_set=artists_set)\n",
    "    x_test = df3.drop('Artist', axis=1)\n",
    "    y_test = df3['Artist']\n",
    "    fts = x_train.columns\n",
    "    \n",
    "    probs = []\n",
    "\n",
    "    for i in artists_set:\n",
    "        X2 = df_maker(artist=i, df=x_train, df2=df2)\n",
    "        X2 = pd.concat([X2.reset_index(drop=True), x_train.iloc[:, :-1].reset_index(drop=True)], axis=1)\n",
    "        X3 = df_maker(artist=i, df=x_test, df2=df2)\n",
    "        X3 = pd.concat([X3.reset_index(drop=True), x_test.iloc[:, :-1].reset_index(drop=True)], axis=1)\n",
    "        y2 = np.array([1 if x == i else 0 for x in y_train])\n",
    "\n",
    "        # Fit logistic regression model with bagging\n",
    "        logreg = LogisticRegression(max_iter=8000, solver='lbfgs', class_weight={0: neg_w, 1: pos_w})\n",
    "        logreg_bagging = BaggingClassifier(estimator=logreg, n_estimators=nest, max_samples=max_sam, max_features=max_fts, oob_score=oob, warm_start=warm)\n",
    "        logreg_bagging.fit(X2, y2)\n",
    "        probs.append(logreg_bagging.predict_proba(X3)[:, 1])\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    probs_by_tops = []\n",
    "\n",
    "    for i in top:\n",
    "        Q = (np.argsort(probs, axis=0)[-i:, :]).transpose()\n",
    "        temp = []\n",
    "        for i in Q:\n",
    "            temp2 = []\n",
    "            for j in i:\n",
    "                temp2.append(artists_set[j])\n",
    "            temp.append(temp2)\n",
    "        probs_by_tops.append(temp)\n",
    "\n",
    "    return top_K_accs(y_test=y_test, arr=probs_by_tops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7c039aaf-6a9a-4e64-b787-ae6bb1899bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_test_random_boosting(artists_set, top, x_train, x_test):\n",
    "    \"\"\"\n",
    "    Perform logistic regression-based artist classification with AdaBoost and evaluate top-K accuracy.\n",
    "\n",
    "    Args:\n",
    "        artists_set (list): List of artists to classify.\n",
    "        top (list): List of integers specifying the top-K values for accuracy evaluation.\n",
    "        x_train (DataFrame): Training feature data.\n",
    "        x_test (DataFrame): Testing feature data.\n",
    "\n",
    "    Returns:\n",
    "        list: List of top-K accuracy scores for each top-K value in 'top'.\n",
    "\n",
    "    Note:\n",
    "        This function performs logistic regression-based artist classification with AdaBoost\n",
    "        for a given set of artists and evaluates the top-K accuracy of the predictions.\n",
    "    \"\"\"\n",
    "    # Filter the training data for the specified artist set\n",
    "    df2, _, pos_w, neg_w = n_sets_func(df=x_train, art_set=artists_set)\n",
    "    x_train = df2.drop('Artist', axis=1)\n",
    "    y_train = df2['Artist']\n",
    "\n",
    "    # Filter the testing data for the specified artist set\n",
    "    df3, _, pos_w, neg_w = n_sets_func(df=x_test, art_set=artists_set)\n",
    "    x_test = df3.drop('Artist', axis=1)\n",
    "    y_test = df3['Artist']\n",
    "    \n",
    "    probs = []\n",
    "\n",
    "    for i in artists_set:\n",
    "        X2 = df_maker(artist=i, df=x_train, df2=df2)\n",
    "        X2 = pd.concat([X2.reset_index(drop=True), x_train.iloc[:, :-1].reset_index(drop=True)], axis=1)\n",
    "        X3 = df_maker(artist=i, df=x_test, df2=df2)\n",
    "        X3 = pd.concat([X3.reset_index(drop=True), x_test.iloc[:, :-1].reset_index(drop=True)], axis=1)\n",
    "        y2 = np.array([1 if x == i else 0 for x in y_train])\n",
    "\n",
    "        # Fit logistic regression model with AdaBoost\n",
    "        adaboost = AdaBoostClassifier(estimator=LogisticRegression(max_iter=5000), n_estimators=50, learning_rate=1)\n",
    "        adaboost.fit(X2, y2)\n",
    "        probs.append(adaboost.predict_proba(X3)[:, 1])\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    probs_by_tops = []\n",
    "\n",
    "    for i in top:\n",
    "        Q = (np.argsort(probs, axis=0)[-i:, :]).transpose()\n",
    "        temp = []\n",
    "        for i in Q:\n",
    "            temp2 = []\n",
    "            for j in i:\n",
    "                temp2.append(artists_set[j])\n",
    "            temp.append(temp2)\n",
    "        probs_by_tops.append(temp)\n",
    "\n",
    "    return top_K_accs(y_test=y_test, arr=probs_by_tops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2394a819-b8c9-4538-a0a1-bd708137a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassy(n, k, l):\n",
    "    \n",
    "    # n : number of artists considered\n",
    "    # k : top k predictions considered\n",
    "    # l : top l misclassified\n",
    "\n",
    "    #Returns:\n",
    "    #list: the top l most misclassified artist numbers by percentage.\n",
    "    #list: the top l least misclassified artist numbers by percentage.\n",
    "    \n",
    "    misclassified_numbers = Counter()\n",
    "    artists_set = random.sample(list(arts_dict_op.keys()), n)\n",
    "    probs_by_tops, accs, _, y_test, _, artists_set = logistic_test(artists_set, top = [k], x_train = num_train,x_test= num_test)\n",
    "\n",
    "    probs_by_tops = probs_by_tops[0]\n",
    "    for i, true_value in enumerate(y_test):\n",
    "        if true_value not in probs_by_tops[i]:\n",
    "            misclassified_numbers[true_value] += 1\n",
    "\n",
    "    total_samples = Counter(y_test)  # Count occurrences of each number in y_test\n",
    "\n",
    "    misclassification_percentages = {\n",
    "        number: count / total_samples[number] * 100\n",
    "        for number, count in misclassified_numbers.items()\n",
    "    }\n",
    "\n",
    "    least_misclassified = sorted(\n",
    "        misclassification_percentages.items(), key=lambda x: x[1]\n",
    "    )[:l]\n",
    "    \n",
    "    top_misclassified = sorted(\n",
    "        misclassification_percentages.items(), key=lambda x: x[1], reverse=True\n",
    "    )[:l]\n",
    "\n",
    "    print(f\"The top {l} most misclassified numbers by percentage are:\\n\")\n",
    "    for number, percentage in top_misclassified:\n",
    "        print(f\"Number: {arts_dict_op[number]}, Misclassification percentage: {percentage:.2f}%\")\n",
    "        \n",
    "    print(f\"The top {l} least misclassified numbers by percentage are:\\n\")\n",
    "    for number, percentage in least_misclassified:\n",
    "        print(f\"Number: {arts_dict_op[number]}, Misclassification percentage: {percentage:.2f}%\")\n",
    "        \n",
    "    return [top[0] for top in top_misclassified], [top[0] for top in least_misclassified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "308d1e3a-c7fb-445d-88fa-f240daa37e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misclassified_top_n(y_test, probs, n):\n",
    "    \"\"\"\n",
    "    Get the top N most frequently misclassified values along with their counts.\n",
    "\n",
    "    Args:\n",
    "        y_test (list or numpy.ndarray): True labels.\n",
    "        probs (list or numpy.ndarray): Predicted probabilities.\n",
    "        n (int): Number of top misclassified values to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains a misclassified value and its count.\n",
    "\n",
    "    Note:\n",
    "        This function takes true labels and predicted probabilities, identifies misclassified values\n",
    "        by comparing them, counts the occurrences of each misclassified value, and returns the top N\n",
    "        misclassified values with the highest counts.\n",
    "    \"\"\"\n",
    "    misclassified = []\n",
    "    \n",
    "    # Iterate over the predictions and true values\n",
    "    for true_val, pred_val in zip(y_test, probs):\n",
    "        if true_val != pred_val:\n",
    "            misclassified.append(pred_val)\n",
    "    \n",
    "    # Count the occurrences of misclassified values\n",
    "    vote_counts = Counter(misclassified)\n",
    "    \n",
    "    # Get the top N misclassified values with the most votes\n",
    "    top_n_misclassified = vote_counts.most_common(n)\n",
    "    \n",
    "    return top_n_misclassified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "651896e2-0f7b-4d2e-8370-d5fed937a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_tog(brr, nlist, top, save_dir):\n",
    "    \"\"\"\n",
    "    Plot multiple arrays of y values with respect to x values and save the plot.\n",
    "\n",
    "    Args:\n",
    "        brr (list of lists): List of lists of y values for each line on the plot.\n",
    "        nlist (list): List of x values corresponding to the y values.\n",
    "        top (list): List of top-K values for labeling the lines.\n",
    "        save_dir (str): Directory where the plot will be saved.\n",
    "\n",
    "    Note:\n",
    "        This function takes a list of lists of y values ('brr'), a list of x values ('nlist'),\n",
    "        a list of top-K values ('top'), and a directory path ('save_dir'). It plots each list\n",
    "        of y values with respect to x values, labels the lines with top-K values, and saves the\n",
    "        plot in the specified directory.\n",
    "    \"\"\"\n",
    "    brr2 = brr\n",
    "\n",
    "    max_len = max(len(sublist) for sublist in brr2)\n",
    "    drr = [[] for _ in range(max_len)]\n",
    "\n",
    "    for sublist in brr2:\n",
    "        for i, element in enumerate(sublist):\n",
    "            drr[i].append(element)\n",
    "\n",
    "    brr2 = drr\n",
    "\n",
    "    counter = 0\n",
    "    for y_values in brr2:\n",
    "        plt.plot(nlist[-len(y_values):], y_values, marker='o', label=\"top \" + str(top[counter]) + \" choices\")\n",
    "        counter += 1\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Number of artists')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy of n artists with top k choices')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    save_path = os.path.join(save_dir, f'top_k_together_test.png')\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "151bd20c-4ba4-45af-abd1-9f04643d0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, st, name):\n",
    "    \"\"\"\n",
    "    Process feature data by aggregating coefficients for features with the same prefix.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Input data containing 'feature' and 'name' columns.\n",
    "        st (str): Prefix of features to be aggregated.\n",
    "        name (str): Name of the coefficient column.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed data as a dictionary containing 'feature' and 'name' columns.\n",
    "\n",
    "    Note:\n",
    "        This function takes a DataFrame 'data' with 'feature' and 'name' columns. It aggregates\n",
    "        coefficients for features that start with the specified prefix 'st' and returns the\n",
    "        processed data as a dictionary with 'feature' and 'name' columns.\n",
    "    \"\"\"\n",
    "    features = data['feature']\n",
    "    coefficients = data[name]\n",
    "    \n",
    "    new_features = []\n",
    "    new_coefficients = []\n",
    "    pos_coefficients = []\n",
    "    pos_total = 0.0\n",
    "    \n",
    "    for feature, coefficient in zip(features, coefficients):\n",
    "        if feature.startswith(st):\n",
    "            pos_coefficients.append(coefficient)\n",
    "        else:\n",
    "            new_features.append(feature)\n",
    "            new_coefficients.append(coefficient)\n",
    "    \n",
    "    if pos_coefficients:\n",
    "        pos_total = sum(pos_coefficients)\n",
    "        real = st\n",
    "        if(st == 'PC' or st =='TF'):\n",
    "            real = \"TF IDF Feat\"\n",
    "        elif(st=='Nb'):\n",
    "            real = \"shape Feat\"\n",
    "        elif(st == 'RI'):\n",
    "            real = 'RID Feat'\n",
    "        elif(st == 'PO'):\n",
    "            real = 'POS Feat'\n",
    "        elif(st==\"Su\"):\n",
    "            real= 'Subjectivity'\n",
    "        elif(st == 'Po'):\n",
    "            real = 'Polarity'\n",
    "        elif(st=='TT'):\n",
    "            real = 'TTR'\n",
    "        elif(st == 'Sl'):\n",
    "            real = 'Slang'\n",
    "        elif(st=='Ra'):\n",
    "            real = \"Rare\"\n",
    "        elif(st=='Li'):\n",
    "            real = \"Line similarity\"\n",
    "        elif(st==\"CH\"):\n",
    "            real = 'Chunk Feat'\n",
    "        elif(st=='Ti'):\n",
    "            real = 'Title in Song'\n",
    "        new_features.append(real)\n",
    "        new_coefficients.append(pos_total)\n",
    "    \n",
    "    processed_data = {\n",
    "        'feature': new_features,\n",
    "        name: new_coefficients\n",
    "    }\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "23676f8a-3763-431d-bc28-a6a82abd794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data, name, b, c):\n",
    "    \"\"\"\n",
    "    Create and display a bar plot of feature importances or coefficients.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary containing feature names and corresponding values.\n",
    "        name (str): Name of the values to be plotted.\n",
    "        b (bool): Whether to adjust figure size and label fonts.\n",
    "        c (int): Plot type indicator (1 for coefficients, 2 for normalized importance, 3 for importance percentage).\n",
    "\n",
    "    Note:\n",
    "        This function takes a dictionary 'data' with feature names as keys and corresponding values,\n",
    "        'name' as the name of the values to be plotted, 'b' as a boolean to adjust figure size and label fonts,\n",
    "        and 'c' as an indicator for the type of plot. It creates and displays a bar plot of feature importances\n",
    "        or coefficients. The plot can be saved as 'ft_importance_test.png' in the current working directory.\n",
    "    \"\"\"\n",
    "    feature_importance_df = pd.DataFrame(data)\n",
    "    sorted_df = feature_importance_df.sort_values(name)\n",
    "    if b:\n",
    "        plt.figure(figsize=(12, 6))  # Set the figure size to be wider (12 units wide, 6 units tall)\n",
    "        x_label_fontsize = 8  # Set the font size for x-axis labels\n",
    "    else:\n",
    "        plt.figure()  # Use default figure size\n",
    "        x_label_fontsize = 12  # Set the font size for x-axis labels\n",
    "    \n",
    "    plt.bar(sorted_df['feature'], sorted_df[name], facecolor='gray', align='center')\n",
    "    plt.xlabel('Feature', fontsize=x_label_fontsize)  # Set the x-axis label font size\n",
    "    plt.ylabel(name)  # Reduce the y-axis label size\n",
    "    if(c==3):\n",
    "        plt.title(f'Importance Percentage in the classification of the features')  # Reduce the title size\n",
    "    if(c==2):\n",
    "        plt.title(f'Importance in the classification of the features (absolute value normalised)')\n",
    "    if(c==1):\n",
    "        plt.title(f'Coefficients in the classification of the features')\n",
    "    plt.xticks(rotation=60, ha='right', fontsize=8)  \n",
    "    plt.yticks(fontsize=8)  \n",
    "    plt.show()\n",
    "    save_dir = os.getcwd()  \n",
    "    save_path = os.path.join(save_dir, 'ft_importance_test.png')\n",
    "    plt.savefig(save_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b6c4278c-d89b-45c4-bfd4-e9f71626a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_imp1(crr, fts):\n",
    "    \"\"\"\n",
    "    Plot feature importances based on coefficients (absolute value thresholded).\n",
    "\n",
    "    Args:\n",
    "        crr (list of lists): List of lists of coefficients for different models or iterations.\n",
    "        fts (list): List of feature names.\n",
    "\n",
    "    Note:\n",
    "        This function takes a list of lists 'crr' containing coefficients for different models or iterations,\n",
    "        and a list of feature names 'fts'. It plots feature importances based on coefficients with an absolute\n",
    "        value threshold of 0.05 using the 'plotter' function.\n",
    "    \"\"\"\n",
    "    for coeffs in crr:\n",
    "        fts2 = [(element + \"_DN\") for element in fts if element != 'Artist']\n",
    "        for i in range(0, len(fts2)):\n",
    "            if (fts2[i] == \"PCA diff normal\"):\n",
    "                fts2[i] = \"TF_IDF vector similarity\"\n",
    "        fts3 = [element for element in fts if element != 'Artist' and element != 'PCA']\n",
    "        fts2 += fts3\n",
    "        fts4 = []\n",
    "        coeffs2 = []\n",
    "        for i in range(0, len(coeffs)):\n",
    "            if (np.abs(coeffs[i]) >= 0.05):\n",
    "                coeffs2.append(coeffs[i])\n",
    "                fts4.append(fts2[i])\n",
    "        data = {'feature': fts4, 'coefficient': coeffs2}\n",
    "        plotter(data, 'coefficient', True, 1)\n",
    "\n",
    "def feature_imp2(crr, fts):\n",
    "    \"\"\"\n",
    "    Plot feature importances based on normalized absolute coefficients (without lyric features).\n",
    "\n",
    "    Args:\n",
    "        crr (list of lists): List of lists of coefficients for different models or iterations.\n",
    "        fts (list): List of feature names.\n",
    "\n",
    "    Note:\n",
    "        This function takes a list of lists 'crr' containing coefficients for different models or iterations,\n",
    "        and a list of feature names 'fts'. It plots feature importances based on normalized absolute coefficients\n",
    "        without considering features starting with 'Ly' using the 'plotter' function.\n",
    "    \"\"\"\n",
    "    for coeffs in crr:\n",
    "        fts2 = [(element + \" diff normal\") for element in fts if element != 'Artist']\n",
    "        fts3 = [element for element in fts if element != 'Artist' and element != 'PCA']\n",
    "        fts2 += fts3\n",
    "        coeffs2 = np.absolute(coeffs)\n",
    "        coeffs2 = coeffs2 / np.linalg.norm(coeffs2)\n",
    "        indices = []\n",
    "        for i in range(0, len(fts2)):\n",
    "            if (fts2[i].startswith('Ly')):\n",
    "                indices.append(i)\n",
    "        result = [x for i, x in enumerate(fts2) if i not in indices]\n",
    "        res = [x for i, x in enumerate(coeffs2) if i not in indices]\n",
    "        data = {'feature': result, 'coefficient': res}\n",
    "\n",
    "        for bruh in fts3:\n",
    "            data = process_data(data, bruh[:2], 'coefficient')\n",
    "        plotter(data, 'coefficient', False, 2)\n",
    "\n",
    "def feature_imp3(crr, fts):\n",
    "    \"\"\"\n",
    "    Plot feature importances based on percentage of normalized absolute coefficients (without lyric features).\n",
    "\n",
    "    Args:\n",
    "        crr (list of lists): List of lists of coefficients for different models or iterations.\n",
    "        fts (list): List of feature names.\n",
    "\n",
    "    Note:\n",
    "        This function takes a list of lists 'crr' containing coefficients for different models or iterations,\n",
    "        and a list of feature names 'fts'. It plots feature importances based on the percentage of normalized\n",
    "        absolute coefficients without considering features starting with 'Ly' using the 'plotter' function.\n",
    "    \"\"\"\n",
    "    for coeffs in crr:\n",
    "        fts2 = [(element + \" diff normal\") for element in fts if element != 'Artist']\n",
    "        fts3 = [element for element in fts if element != 'Artist' and element != 'PCA']\n",
    "        fts2 += fts3\n",
    "        coeffs2 = np.absolute(coeffs)\n",
    "        coeffs2 = coeffs2 / np.linalg.norm(coeffs2)\n",
    "        coeffs3 = 100 * coeffs2 / np.sum(coeffs2)\n",
    "        indices = []\n",
    "        for i in range(0, len(fts2)):\n",
    "            if (fts2[i].startswith('Ly')):\n",
    "                indices.append(i)\n",
    "        result = [x for i, x in enumerate(fts2) if i not in indices]\n",
    "        res = [x for i, x in enumerate(coeffs3) if i not in indices]\n",
    "        data = {'feature': result, 'Percentage': res}\n",
    "        for bruh in fts3:\n",
    "            if (bruh.startswith('Ly')):\n",
    "                continue\n",
    "            data = process_data(data, bruh[:2], 'Percentage')\n",
    "        plotter(data, 'Percentage', False, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d589b5bd-254a-44d4-bb74-61dc74858919",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     13\u001b[0m     aset \u001b[38;5;241m=\u001b[39m  random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mlist\u001b[39m(arts_in_dataset), n) \n\u001b[1;32m---> 14\u001b[0m     _,br,cr,_,_,_ \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43martists_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(br[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m>\u001b[39mmaxbr[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     16\u001b[0m         maxbr \u001b[38;5;241m=\u001b[39m br\n",
      "Cell \u001b[1;32mIn[126], line 48\u001b[0m, in \u001b[0;36mlogistic_test\u001b[1;34m(artists_set, top, x_train, x_test)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Fit logistic regression model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m logreg \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: neg_w, \u001b[38;5;241m1\u001b[39m: pos_w})\n\u001b[1;32m---> 48\u001b[0m \u001b[43mlogreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m logreg\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[0;32m     50\u001b[0m coeffs\u001b[38;5;241m.\u001b[39mappend(coefficients[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1289\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1291\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    446\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[0;32m    447\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    448\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    449\u001b[0m ]\n\u001b[1;32m--> 450\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    459\u001b[0m     solver,\n\u001b[0;32m    460\u001b[0m     opt_res,\n\u001b[0;32m    461\u001b[0m     max_iter,\n\u001b[0;32m    462\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    463\u001b[0m )\n\u001b[0;32m    464\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    353\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:278\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[1;32m--> 278\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    285\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_penalty(weights, l2_reg_strength)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\_loss\\loss.py:257\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[1;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m ReadonlyArrayWrapper(sample_weight)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nlist=[128]\n",
    "brr=[]\n",
    "brrlog=[]\n",
    "crr=[]\n",
    "top=[]\n",
    "fts = np.array(train_df.columns)\n",
    "for n in nlist:\n",
    "    maxbr=[0]\n",
    "    maxcr=0\n",
    "    maxbrlog=[0]\n",
    "    top =[2**i for i in range(0, (int)(np.log2(n//2)+1))]\n",
    "    for i in range(0,1):\n",
    "        aset =  random.sample(list(arts_in_dataset), n) \n",
    "        _,br,cr,_,_,_ = logistic_test(artists_set=aset,top=top, x_train = num_train, x_test = num_test)\n",
    "        if(br[0]>maxbr[0]):\n",
    "            maxbr = br\n",
    "            maxcr = cr\n",
    "        a = logistic_test_random_bagging(artists_set=aset, top = top, nest=10, x_train = num_train, x_test = num_test, max_sam = 1.0, max_fts = 1.0, oob = False, warm = True)\n",
    "        print(a)\n",
    "        if(a[0]>maxbrlog[0]):\n",
    "            maxbrlog = a\n",
    "    brr.append(maxbr)\n",
    "    crr.append(maxcr)\n",
    "    brrlog.append(maxbrlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb0186-73c6-4e06-9320-c83bde6f76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE THIS\n",
    "\n",
    "brrsave = brr\n",
    "crrsave = crr\n",
    "brrlogsave = brrlog\n",
    "\n",
    "topsave = [1,2,4,8,16,32]\n",
    "bs2 = brrsave\n",
    "cs2 = crrsave\n",
    "bls2 = brrlogsave\n",
    "ts2 = topsave\n",
    "\n",
    "brrsave = bs2\n",
    "crrsave = cs2\n",
    "brrlogsave = bls2\n",
    "topsave = ts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341c53c-5119-459c-8052-d352706b4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_dict = {}  # Dictionary to store feature families and their variances\n",
    "\n",
    "for feature in numerical_df_scaled.columns:\n",
    "    if feature[:2] in {\"Ar\", \"Mi\"}:\n",
    "        continue\n",
    "    if feature[:2] == \"PO\":\n",
    "        family = \"POS feat\"\n",
    "    elif feature[:2] == \"CH\":\n",
    "        family = \"CHUNK feat\"\n",
    "    elif feature[:2] == \"RI\":\n",
    "        family = \"RID feat\"\n",
    "    else:\n",
    "        family = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c5b14-41f6-4b05-8fc3-621de031f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp1(crrsave, fts)\n",
    "feature_imp2(crrsave, fts)\n",
    "feature_imp3(crrsave, fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37d309-3c17-4881-990f-2fbbca182027",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_tog(brr = brr, nlist =nlist, top = top)\n",
    "plot_top_tog(brr = brrlog, nlist =nlist, top = top)\n",
    "feature_imp1(crr=crr, fts = fts)\n",
    "feature_imp2(crr=crr, fts = fts)\n",
    "feature_imp3(crr=crr, fts = fts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
